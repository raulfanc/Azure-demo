{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark job on Databricks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ShortType, DateType\n",
    "from pyspark.sql.functions import col, datediff\n",
    "\n",
    "# define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Rego\", StringType(), True),\n",
    "    StructField(\"Brand\", StringType(), True),\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"Trim\", StringType(), True),\n",
    "    StructField(\"Year\", ShortType(), True),\n",
    "    StructField(\"Odometer\", IntegerType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True),\n",
    "    StructField(\"Date listed\", DateType(), True),\n",
    "    StructField(\"Date removed\", DateType(), True),\n",
    "    StructField(\"Turnover\", ShortType(), True)\n",
    "])\n",
    "\n",
    "# Read the JSON files into a Spark DataFrame\n",
    "df = spark.read.json(\"/mnt/bronze\", schema=schema)\n",
    "\n",
    "# Apply transformations\n",
    "df = df.withColumn(\"Turnover\", datediff(col(\"Date removed\"), col(\"Date listed\")).cast(\"int\"))\n",
    "# Apply transformations\n",
    "df = df.withColumn(\"Turnover\", datediff(col(\"Date removed\"), col(\"Date listed\")).cast(\"int\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect mount\n",
    "len(dbutils.fs.ls('/mnt/bronze'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ShortType, DateType\n",
    "from pyspark.sql.functions import col, datediff\n",
    "\n",
    "# define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Rego\", StringType(), True),\n",
    "    StructField(\"Brand\", StringType(), True),\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"Trim\", StringType(), True),\n",
    "    StructField(\"Year\", ShortType(), True),\n",
    "    StructField(\"Odometer\", IntegerType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True),\n",
    "    StructField(\"Date listed\", DateType(), True),\n",
    "    StructField(\"Date removed\", DateType(), True),\n",
    "    StructField(\"Turnover\", ShortType(), True)\n",
    "])\n",
    "\n",
    "# Read and Join the JSON files into a Spark DataFrame\n",
    "df = spark.read.json(\"/mnt/bronze\", schema=schema)\n",
    "\n",
    "# Apply transformations\n",
    "df = df.withColumn(\"Turnover\", datediff(col(\"Date removed\"), col(\"Date listed\")).cast(\"int\"))\n",
    "df = df.orderBy(\"Brand\")\n",
    "\n",
    "# preview the dataframe\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mount silver folder --> Processed Zone\n",
    "\n",
    "adlsFolderName = \"silver\"\n",
    "mountPoint_silver = \"/mnt/silver\"\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://\" + adlsContainerName + \"@\" + adlsAccountName + \".blob.core.windows.net/\" + adlsFolderName,\n",
    "  mount_point = mountPoint_silver,\n",
    "  extra_configs = {\"fs.azure.account.key.\" + adlsAccountName + \".blob.core.windows.net\":'ntTRTeNT+btriZRlZnfiVOifYio2WRPF7RY/fJKsrm4MM1b5rXKyp1oFWNgLULllaDO1hid1u5cd+ASt3SEDug=='}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet output\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"processed_{now}.parquet\"\n",
    "df.write.parquet(f\"/mnt/silver/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV output\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"processed_{now}.csv\"\n",
    "df.write.format(\"csv\").option(\"header\", True).mode(\"overwrite\").save(f\"/mnt/silver/{filename}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code below invalid "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because Databricks doesn't support wildcard search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mount archive folder --> cold storage\n",
    "adlsAccountName = \"demodatalake23020801\"\n",
    "adlsContainerName = \"demo-container\"\n",
    "adlsFolderName = \"archive\"\n",
    "mountPoint_archive = \"/mnt/archive\"\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://\" + adlsContainerName + \"@\" + adlsAccountName + \".blob.core.windows.net/\" + adlsFolderName,\n",
    "  mount_point = mountPoint_archive,\n",
    "  extra_configs = {\"fs.azure.account.key.\" + adlsAccountName + \".blob.core.windows.net\":'ntTRTeNT+btriZRlZnfiVOifYio2WRPF7RY/fJKsrm4MM1b5rXKyp1oFWNgLULllaDO1hid1u5cd+ASt3SEDug=='}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of files in the /mnt/bronze directory\n",
    "files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "\n",
    "# Loop through the files and move the ones with the name format new_listing_*\n",
    "for file in files:\n",
    "    if file.name.startswith(\"new_listing_\"):\n",
    "        dbutils.fs.mv(file.path, \"/mnt/archive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get a list of files in the /mnt/bronze directory\n",
    "files = dbutils.fs.ls(\"/mnt/bronze\")\n",
    "\n",
    "# Loop through the files and move the ones with the name format new_listing_*\n",
    "for file in files:\n",
    "    if os.path.basename(file.path).startswith(\"new\"):\n",
    "        dbutils.fs.mv(file.path, \"/mnt/archive/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d187f2e9dee7598edc4a688ec32edfe3d7a2f237f4345d12f00c29e3ea6101bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
